{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sim: Gene Regulatory Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dbm829/projects/interpreting_omics_models/02_experiments/notebooks\n",
      "/home/dbm829/projects/interpreting_omics_models\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# print directory\n",
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"../..\")\n",
    "print(os.getcwd())\n",
    "\n",
    "from src.models.sparse_autoencoder import *\n",
    "from src.visualization.plotting import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# set a random seed\n",
    "seed = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from files.\n",
      "RNA shape: torch.Size([12000, 5])\n",
      "activity score shape: torch.Size([12000, 1])\n",
      "accessibility score shape: torch.Size([12000, 3])\n"
     ]
    }
   ],
   "source": [
    "# setup:\n",
    "# 3 genes, transcribed into mRNA and translated into proteins\n",
    "# the randomness comes from 2 places:\n",
    "# 1. sampling from a poisson distribution, indicating the level of transcription activity\n",
    "# 2. sampling from bernoulli distributions for each gene, indicating whether the gene is even accessible or not\n",
    "\n",
    "# set up experiment parameters\n",
    "poisson_lambda = 2.0\n",
    "p_bernoulli = torch.tensor([0.5, 0.1, 0.9])\n",
    "gene_regulation = torch.tensor([\n",
    "    [0., 1., 1., 0., 0.],\n",
    "    [1., 1., 0., 1., 0.],\n",
    "    [0., 0., 0., 1., 1.]\n",
    "])\n",
    "\n",
    "n_samples_validation = 2000\n",
    "n_samples = 10000 + n_samples_validation\n",
    "n_epochs = 20000\n",
    "\n",
    "# load or generate the data\n",
    "data_path = \"01_data/\"\n",
    "\n",
    "if os.path.exists(data_path + \"sim_rna_counts.npy\"):\n",
    "    rna_counts = torch.tensor(np.load(data_path + \"sim_rna_counts.npy\"))\n",
    "    tf_scores = torch.tensor(np.load(data_path + \"sim_tf_scores.npy\"))\n",
    "    activity_score = torch.tensor(np.load(data_path + \"sim_activity_scores.npy\"))\n",
    "    accessibility_scores = torch.tensor(np.load(data_path + \"sim_accessibility_scores.npy\"))\n",
    "\n",
    "    print(\"Loaded data from files.\")\n",
    "else:\n",
    "    # init distributions\n",
    "    activity_distribution = torch.distributions.poisson.Poisson(torch.tensor([poisson_lambda]))\n",
    "    accessibility_distribution = torch.distributions.bernoulli.Bernoulli(p_bernoulli)\n",
    "\n",
    "    # sample from distributions to create mRNA counts\n",
    "    # get the activity score\n",
    "    activity_score = activity_distribution.sample((n_samples,))\n",
    "    # get the accessibility scores\n",
    "    accessibility_scores = accessibility_distribution.sample((n_samples,))\n",
    "    # get the mRNA counts\n",
    "    tf_scores = activity_score * accessibility_scores\n",
    "\n",
    "    # correct mRNA counts for gene regulation\n",
    "    # first, get the regulation scores\n",
    "    def get_regulation_scores(tf_scores, gene_regulation):\n",
    "        # get the regulation scores\n",
    "        return torch.matmul(tf_scores, gene_regulation)\n",
    "\n",
    "    rna_counts = get_regulation_scores(tf_scores, gene_regulation)\n",
    "\n",
    "    \"\"\"\n",
    "    np.save(\"01_data/sim_rna_counts.npy\", rna_counts.numpy())\n",
    "    np.save(\"01_data/sim_tf_scores.npy\", tf_scores.numpy())\n",
    "    np.save(\"01_data/sim_activity_scores.npy\", activity_score.numpy())\n",
    "    np.save(\"01_data/sim_accessibility_scores.npy\", accessibility_scores.numpy())\n",
    "    \"\"\"\n",
    "\n",
    "print(f'RNA shape: {rna_counts.shape}')\n",
    "print(f'activity score shape: {activity_score.shape}') # one per sample\n",
    "print(f'accessibility score shape: {accessibility_scores.shape}') # one per originating gene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_model(encoder, decoder, rna_counts, n_samples, n_samples_validation, learning_rate=1e-4):\n",
    "    # loss function\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=learning_rate)\n",
    "\n",
    "    # train\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        # train\n",
    "        # forward pass\n",
    "        encoded = encoder(rna_counts[:(n_samples-n_samples_validation)])\n",
    "        decoded = decoder(encoded)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(decoded, rna_counts[:(n_samples-n_samples_validation)])\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # validation\n",
    "        # forward pass\n",
    "        encoded = encoder(rna_counts[(n_samples-n_samples_validation):])\n",
    "        decoded = decoder(encoded)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(decoded, rna_counts[(n_samples-n_samples_validation):])\n",
    "        val_loss.append(loss.item())\n",
    "    \n",
    "    # plot loss curves\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(train_loss, label='train')\n",
    "    plt.plot(val_loss, label='validation')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # make scatter plots for each gene with output vs input\n",
    "    # get the output\n",
    "    encoded = encoder(rna_counts)\n",
    "    decoded = decoder(encoded)\n",
    "    # plot subplots\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(20, 5))\n",
    "    for i in range(5):\n",
    "        axs[i].scatter(rna_counts[:, i].detach().numpy(), decoded[:, i].detach().numpy(), s=1)\n",
    "        axs[i].set_xlabel('input')\n",
    "        axs[i].set_ylabel('output')\n",
    "        axs[i].set_title('gene {}'.format(i))\n",
    "    plt.show()\n",
    "\n",
    "    # rank underlying features by their importance\n",
    "    # first, get the weights of the encoder\n",
    "    encoder_weights = encoder[0].weight.detach().t().numpy()\n",
    "    # then, get the weights of the decoder\n",
    "    decoder_weights = decoder[-2].weight.detach().t().numpy()\n",
    "    # multiply them together\n",
    "    weights = np.matmul(encoder_weights, decoder_weights)\n",
    "    # get the absolute values\n",
    "    weights = np.abs(weights)\n",
    "    # sum the weights for each gene\n",
    "    weights = np.sum(weights, axis=1)\n",
    "    # sort the weights\n",
    "    #weights = np.argsort(weights)\n",
    "\n",
    "    # plot the weights\n",
    "    plt.bar(np.arange(5), weights)\n",
    "    plt.xlabel('gene')\n",
    "    plt.ylabel('weight')\n",
    "    plt.title('gene importance')\n",
    "    plt.show()\n",
    "\n",
    "    history = pd.DataFrame({'train_loss': train_loss, 'val_loss': val_loss, 'epoch': np.arange(n_epochs)})\n",
    "\n",
    "    return encoder, decoder, history\n",
    "\n",
    "three_cols = [\"#60276F\", \"#1663A5\", \"#53787B\"]\n",
    "fontsize = 12\n",
    "\n",
    "def plot_neuron_layer(activations, corr_threshold=0.9, name=''):\n",
    "    # make the font size smaller\n",
    "    plt.rcParams.update({'font.size': fontsize})\n",
    "    if activations.shape[1] > 10:\n",
    "        # look for neurons that correlate strongly with a gene\n",
    "        # first, get the correlations\n",
    "        for i in range(activations.shape[1]):\n",
    "            for j in range(tf_scores.shape[1]):\n",
    "                corr = np.corrcoef(activations[:, i].detach().numpy(), tf_scores[:, j].detach().numpy())[0, 1]\n",
    "                if corr > corr_threshold:\n",
    "                    # plot the neuron\n",
    "                    plt.scatter(activations[:, i].detach().numpy(), tf_scores[:, j].detach().numpy())\n",
    "                    plt.xlabel('neuron {}'.format(i))\n",
    "                    plt.ylabel('tf_score {}'.format(j))\n",
    "                    plt.title('correlation: {}'.format(corr))\n",
    "                    plt.legend().remove()\n",
    "                    plt.show()\n",
    "    else:\n",
    "        # now plot each activation against each tf_score\n",
    "        fig, axs = plt.subplots(activations.shape[1], 3, figsize=(9, 3*activations.shape[1]))\n",
    "        for i in range(activations.shape[1]):\n",
    "            for j in range(3):\n",
    "                # make a dataframe\n",
    "                df = pd.DataFrame({'neuron': activations[:, i].detach().numpy(), 'tf_score': tf_scores[:, j].detach().numpy(), 'activity': activity_score.flatten().detach().numpy()})\n",
    "                sns.scatterplot(x='neuron', y='tf_score', hue='activity', data=df, ax=axs[i, j])\n",
    "                axs[i, j].set_xlabel('neuron {}'.format(i))\n",
    "                axs[i, j].set_ylabel('tf_score {}'.format(j))\n",
    "                axs[i, j].legend().remove()\n",
    "        plt.show()\n",
    "\n",
    "    # for tf_score 0, fit a linear regression model to the activations\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    fig, axs = plt.subplots(1, tf_scores.shape[1], figsize=(10, 3))\n",
    "    # add spacing\n",
    "    fig.subplots_adjust(wspace=0.5)\n",
    "    for i in range(tf_scores.shape[1]):\n",
    "        reg = LinearRegression().fit(activations.detach().numpy(), tf_scores[:, i].detach().numpy())\n",
    "        print(reg.score(activations.detach().numpy(), tf_scores[:, i].detach().numpy()))\n",
    "        # use these coefficients to plot the activations against the tf_scores\n",
    "        superposition = np.matmul(activations.detach().numpy(), reg.coef_.T)\n",
    "        # now plot the tf_score against the superposition\n",
    "        axs[i].scatter(superposition, tf_scores[:, i].detach().numpy(), c=three_cols[i])\n",
    "        axs[i].set_xlabel('superposition')\n",
    "        axs[i].set_ylabel('TF score {}'.format(i))\n",
    "        # the title should be the coefficients\n",
    "        axs[i].set_title('coefficients: {}'.format(np.round(reg.coef_,2)), fontsize=fontsize-2)\n",
    "    # save as pdf\n",
    "    plt.savefig('tf_score_superposition'+name+'.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # also find the superposition for the activity score\n",
    "    reg = LinearRegression().fit(activations.detach().numpy(), activity_score.flatten().detach().numpy())\n",
    "    print(reg.score(activations.detach().numpy(), activity_score.flatten().detach().numpy()))\n",
    "    # use these coefficients to plot the activations against the tf_scores\n",
    "    superposition = np.matmul(activations.detach().numpy(), reg.coef_.T)\n",
    "    # now plot the tf_score against the superposition\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(3, 3))\n",
    "    plt.scatter(superposition, activity_score.flatten().detach().numpy(), c='black')\n",
    "    plt.xlabel('superposition')\n",
    "    plt.ylabel('activity_score')\n",
    "    # the title should be the coefficients\n",
    "    plt.title('coefficients: {}'.format(np.round(reg.coef_,2)), fontsize=fontsize)\n",
    "    # save as pdf\n",
    "    plt.savefig('activity_score_superposition'+name+'.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # make one plot showing a pca with the coefficient vectors for each tf score\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(3, 3))\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(activations.detach().numpy())\n",
    "    # get the coefficients\n",
    "    coefficients = []\n",
    "    for i in range(tf_scores.shape[1]):\n",
    "        reg = LinearRegression().fit(activations.detach().numpy(), tf_scores[:, i].detach().numpy())\n",
    "        coefficients.append(reg.coef_)\n",
    "    # add the activity score coefficients\n",
    "    reg = LinearRegression().fit(activations.detach().numpy(), activity_score.flatten().detach().numpy())\n",
    "    coefficients.append(reg.coef_)\n",
    "    coefficients = np.array(coefficients)\n",
    "    # transform the coefficients\n",
    "    coefficients = pca.transform(coefficients)\n",
    "    # plot the coefficients\n",
    "    plt.scatter(coefficients[:, 0], coefficients[:, 1], s=1)\n",
    "    # label the coefficients\n",
    "    #for i in range(coefficients.shape[0]):\n",
    "    #    plt.annotate('TF score {}'.format(i), (coefficients[i, 0]+0.05, coefficients[i, 1]*1.1))\n",
    "    # plot arrows from the origin to the coefficients\n",
    "    mean = pca.transform(pca.mean_.reshape(1, -1))\n",
    "    for i in range(coefficients.shape[0]):\n",
    "        if i == coefficients.shape[0]-1:\n",
    "            plt.arrow(mean[0][0], mean[0][1], coefficients[i, 0], coefficients[i, 1], head_width=0.05, head_length=0.1, fc='black', ec='black')\n",
    "        else:\n",
    "            plt.arrow(mean[0][0], mean[0][1], coefficients[i, 0], coefficients[i, 1], head_width=0.05, head_length=0.1, fc=three_cols[i], ec=three_cols[i])\n",
    "    plt.xlabel('PC 1')\n",
    "    plt.ylabel('PC 2')\n",
    "    plt.title('TF score coefficients', fontsize=fontsize)\n",
    "    # save as pdf\n",
    "    plt.savefig('tf_score_coefficients'+name+'.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # do the same for the activity score and the accessibility scores\n",
    "    if activations.shape[1] > 10:\n",
    "        # look for neurons that correlate strongly with a gene\n",
    "        # first, get the correlations\n",
    "        for i in range(activations.shape[1]):\n",
    "            for j in range(accessibility_scores.shape[1]):\n",
    "                corr = np.corrcoef(activations[:, i].detach().numpy(), accessibility_scores[:, j].detach().numpy())[0, 1]\n",
    "                if corr > corr_threshold:\n",
    "                    # plot the neuron\n",
    "                    plt.scatter(activations[:, i].detach().numpy(), accessibility_scores[:, j].detach().numpy())\n",
    "                    plt.xlabel('neuron {}'.format(i))\n",
    "                    plt.ylabel('accessibility_score {}'.format(j))\n",
    "                    plt.title('correlation: {}'.format(corr))\n",
    "                    plt.legend().remove()\n",
    "                    plt.show()\n",
    "    else:\n",
    "        fig, axs = plt.subplots(activations.shape[1], accessibility_scores.shape[1], figsize=(9, 3*activations.shape[1]))\n",
    "        for i in range(activations.shape[1]):\n",
    "            for j in range(accessibility_scores.shape[1]):\n",
    "                # make a dataframe\n",
    "                df = pd.DataFrame({'neuron': activations[:, i].detach().numpy(), 'accessibility_score': accessibility_scores[:, j].detach().numpy(), 'activity': activity_score.flatten().detach().numpy()})\n",
    "                sns.scatterplot(x='neuron', y='accessibility_score', hue='activity', data=df, ax=axs[i, j])\n",
    "                axs[i, j].set_xlabel('neuron {}'.format(i))\n",
    "                axs[i, j].set_ylabel('accessibility_score {}'.format(j))\n",
    "                axs[i, j].legend().remove()\n",
    "        plt.show()\n",
    "    fig, axs = plt.subplots(1, accessibility_scores.shape[1], figsize=(9, 3))\n",
    "    for i in range(accessibility_scores.shape[1]):\n",
    "        reg = LinearRegression().fit(activations.detach().numpy(), accessibility_scores[:, i].detach().numpy())\n",
    "        # use these coefficients to plot the activations against the tf_scores\n",
    "        superposition = np.matmul(activations.detach().numpy(), reg.coef_.T)\n",
    "        # now plot the tf_score against the superposition\n",
    "        axs[i].scatter(superposition, accessibility_scores[:, i].detach().numpy())\n",
    "        axs[i].set_xlabel('superposition')\n",
    "        axs[i].set_ylabel('accessibility_score {}'.format(i))\n",
    "        # the title should be the coefficients\n",
    "        axs[i].set_title('coefficients: {}'.format(reg.coef_))\n",
    "    plt.show()\n",
    "\n",
    "    fig, axs = plt.subplots(1, activations.shape[1], figsize=(activations.shape[1]*3, 3))\n",
    "    for i in range(activations.shape[1]):\n",
    "        # make a dataframe\n",
    "        axs[i].scatter(activations[:, i].detach().numpy(), activity_score.flatten().detach().numpy())\n",
    "        axs[i].set_xlabel('neuron {}'.format(i))\n",
    "        axs[i].set_ylabel('activity_score')\n",
    "    plt.show()\n",
    "    reg = LinearRegression().fit(activations.detach().numpy(), activity_score.flatten().detach().numpy())\n",
    "    # use these coefficients to plot the activations against the tf_scores\n",
    "    superposition = np.matmul(activations.detach().numpy(), reg.coef_.T)\n",
    "    # now plot the tf_score against the superposition\n",
    "    plt.scatter(superposition, activity_score.flatten().detach().numpy())\n",
    "    plt.xlabel('superposition')\n",
    "    plt.ylabel('activity_score')\n",
    "    # the title should be the coefficients\n",
    "    plt.title('coefficients: {}'.format(reg.coef_))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp1: Superpositions depending on the bottleneck size (relative to causal variables) for single-layer networks\n",
    "\n",
    "Here, the number of causal variables are 4: 3 TFs and the activity in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overcomplete case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(5, 10)\n",
    ")\n",
    "decoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10, 5),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "\n",
    "encoder, decoder, history = train_and_eval_model(encoder, decoder, rna_counts, n_samples, n_samples_validation)\n",
    "\n",
    "# save this model as the best one\n",
    "model_name = 'layer1_latent10_v1'\n",
    "\n",
    "torch.save(encoder, '03_results/models/sim1_'+model_name+'_encoder.pth')\n",
    "torch.save(decoder, '03_results/models/sim1_'+model_name+'_decoder.pth')\n",
    "\n",
    "# also save the history of the training\n",
    "history.to_csv('03_results/models/sim1_'+model_name+'_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ideal case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(5, 4)\n",
    ")\n",
    "decoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 5),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "\n",
    "encoder, decoder, history = train_and_eval_model(encoder, decoder, rna_counts, n_samples, n_samples_validation)\n",
    "\n",
    "# save this model as the best one\n",
    "model_name = 'layer1_latent4_v2'\n",
    "\n",
    "torch.save(encoder, '03_results/models/sim1_'+model_name+'_encoder.pth')\n",
    "torch.save(decoder, '03_results/models/sim1_'+model_name+'_decoder.pth')\n",
    "\n",
    "# also save the history of the training\n",
    "history.to_csv('03_results/models/sim1_'+model_name+'_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bottleneck smaller than the number of causal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(5, 2)\n",
    ")\n",
    "decoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 5),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "\n",
    "encoder, decoder, history = train_and_eval_model(encoder, decoder, rna_counts, n_samples, n_samples_validation)\n",
    "\n",
    "# save this model as the best one\n",
    "model_name = 'layer1_latent2_v1'\n",
    "\n",
    "torch.save(encoder, '03_results/models/sim1_'+model_name+'_encoder.pth')\n",
    "torch.save(decoder, '03_results/models/sim1_'+model_name+'_decoder.pth')\n",
    "\n",
    "# also save the history of the training\n",
    "history.to_csv('03_results/models/sim1_'+model_name+'_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp 2: 2 layers with nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "def custom_weight_init(m, init_option='kaiming'):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        if init_option == 'xavier':\n",
    "            init.xavier_uniform_(m.weight)\n",
    "        elif init_option == 'uniform':\n",
    "            init.uniform_(m.weight, -0.1, 0.1)\n",
    "        elif init_option == 'normal':\n",
    "            init.normal_(m.weight, 0, 0.1)\n",
    "        elif init_option == 'kaiming_uniform':\n",
    "            init.kaiming_uniform_(m.weight)\n",
    "        else:\n",
    "            init.kaiming_normal_(m.weight)\n",
    "        \n",
    "        if m.bias is not None:\n",
    "            init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overcomplete case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_type = 'xavier'\n",
    "\n",
    "encoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(5, 10),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(10, 100),\n",
    ")\n",
    "encoder.apply(lambda m: custom_weight_init(m, init_option=init_type))\n",
    "\n",
    "decoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(100, 10),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(10, 5),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "decoder.apply(lambda m: custom_weight_init(m, init_option=init_type))\n",
    "\n",
    "encoder, decoder = train_and_eval_model(encoder, decoder, rna_counts, n_samples, n_samples_validation)\n",
    "\n",
    "# save this model as the best one\n",
    "model_name = 'layer2_latent100_xavier'\n",
    "\n",
    "torch.save(encoder, '03_results/models/sim1_'+model_name+'_encoder.pth')\n",
    "torch.save(decoder, '03_results/models/sim1_'+model_name+'_decoder.pth')\n",
    "\n",
    "# also save the history of the training\n",
    "history.to_csv('03_results/models/sim1_'+model_name+'_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_type = 'xavier'\n",
    "\n",
    "encoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(5, 5),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(5, 4),\n",
    ")\n",
    "encoder.apply(lambda m: custom_weight_init(m, init_option=init_type))\n",
    "\n",
    "decoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 5),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(5, 5),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "decoder.apply(lambda m: custom_weight_init(m, init_option=init_type))\n",
    "\n",
    "encoder, decoder = train_and_eval_model(encoder, decoder, rna_counts, n_samples, n_samples_validation, learning_rate=1e-3)\n",
    "\n",
    "# save this model as the best one\n",
    "model_name = 'layer2_latent4_xavier_v2'\n",
    "\n",
    "torch.save(encoder, '03_results/models/sim1_'+model_name+'_encoder.pth')\n",
    "torch.save(decoder, '03_results/models/sim1_'+model_name+'_decoder.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bottleneck smaller than the number of causal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_type = 'xavier'\n",
    "\n",
    "encoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(5, 5),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(5, 2),\n",
    ")\n",
    "encoder.apply(lambda m: custom_weight_init(m, init_option=init_type))\n",
    "\n",
    "decoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 5),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(5, 5),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "decoder.apply(lambda m: custom_weight_init(m, init_option=init_type))\n",
    "\n",
    "encoder, decoder = train_and_eval_model(encoder, decoder, rna_counts, n_samples, n_samples_validation, learning_rate=1e-3)\n",
    "\n",
    "# save this model as the best one\n",
    "model_name = 'layer2_latent2_xavier'\n",
    "\n",
    "torch.save(encoder, '03_results/models/sim1_'+model_name+'_encoder.pth')\n",
    "torch.save(decoder, '03_results/models/sim1_'+model_name+'_decoder.pth')\n",
    "\n",
    "# also save the history of the training\n",
    "history.to_csv('03_results/models/sim1_'+model_name+'_history.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc_mechinterp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
